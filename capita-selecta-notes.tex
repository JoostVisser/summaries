\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\PassOptionsToPackage{hyphens}{url} % url is loaded by hyperref
\usepackage[unicode=true]{hyperref}
\hypersetup{
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother


\date{}

\begin{document}

\section{Capita Selecta notes}\label{header-n3}

\tableofcontents

\subsection{Research}\label{header-n6}

\paragraph{Preliminaries}\label{header-n402}

\begin{itemize}
\item
  If it doesn't work, install Anaconda1.
\item
  Install Python 2.7.
\item
  Install CUDNN.
\end{itemize}

\paragraph{Python files}\label{header-n442}

\subparagraph{Class\_shape\_1.py}\label{header-n443}

\subsection{Papers}\label{header-n411}

\subsubsection{Fully Convolutional Networks for Semantic
Segmentation}\label{header-n10}

\paragraph{Definitions}\label{header-n11}

\textbf{Segmentation:} a partition of an image into several "coherent"
parts, without understanding what each part is.

\textbf{Semantic Segmentation:} partition the image into semantically
meaningful parts + classify each part into one of the pre-determined
classes.

\textbf{End-to-end learning:} Learn the whole neural network in one go,
instead of dividing the network into a pipeline of smaller networks.

\textbf{Inference:} Understanding an image. (Done by feedfoward)

\paragraph{Abstract}\label{header-n20}

Fully convolutional networks trained by themselves, trained end-to-end
and pixels-to-pixels, exceed state-in-the-art in \emph{semantic
segmentation}.

\begin{itemize}
\item
  Train "fully convolutional" networks of arbitrary size input and gives
  a similar output.
\item
  Trains very fast.
\end{itemize}

\paragraph{1. Introduction}\label{header-n30}

Prior work tried to make a prediction at every pixel, but these have
shortcomings.

Fully Convolutional Network (FCN) don't have complicated machinery and
work whole-image-at-a-time by dense feedforward computation +
backpropagation

\begin{itemize}
\item
  Up-sampling layers enable pixelwise prediction and learning in nets
  with \emph{subsampled pooling}.
\end{itemize}

\emph{Global information:} stores the \emph{what} of an object.
\emph{Local information:} stores the \emph{where} of an object.

A \textbf{skip achitecture} has been defined to combine deep, coarse
semantic info with shallow, fine appearance information.
{[}Segmentation, not important?{]}

\paragraph{2. Related Work}\label{header-n44}

Skipping this part for now as the next part is more important.

\paragraph{3. Fully convolutional networks}\label{header-n47}

Convolutional network input: \(h \times w \times d\).

\begin{itemize}
\item
  \(h \times w\) is the pixel size of the image.
\item
  \(d\) colour channels. (RBW)
\end{itemize}

Convolutional layer is \(h \times w \times d\).

\begin{itemize}
\item
  \(h \times w\) are spatial dimensions, i.e. related to the location of
  the image.
\item
  \(d\) features. Also called \textbf{channel dimensions}.
\item
  Locations in these layers correspond to the locations in the
  \emph{input image} to which this layer is path-connected to.
\end{itemize}

These are built on \emph{translation invariance}, meaning that their
basic components operate on local input regions and only depend on
\emph{relative} spatial coordinates.

\(\mathbf x_{ij}\) is the data vector at location \((i, j)\).
\(\mathbf y_{ij}\) is the output of this vector, i.e. the data vector
for the following layer:

\[\mathbf y_{ij} = f_{ks}(\{\mathbf x_{s_i + \delta_i,s_j + \delta_j}\}, \forall  0 \leq \delta_i , \delta_j \leq k)\]

\begin{itemize}
\item
  \(s\) is the stride
\item
  \(\delta\) is the index that grabs the current local receptive field
  of size \(k \times k\).
\item
  \(f_{ks}\) determines the layer time:

  \begin{itemize}
  \item
    Matrix multiplication for standard convolution, i.e. logistic
    regression
  \item
    Spatial max for max pooling
  \item
    etc
  \end{itemize}
\end{itemize}

General deep neural network: compute a nonlinear function. Net with only
convolutional layers: compute a nonlinear \emph{filter} (i.e. heatmap).

Still, what loss function to use? How to compute
\(\ell(\mathbf x; \theta)\)?

\begin{itemize}
\item
  Apparently, if we can write the loss function as a sum of the loss of
  each individual pixel, i.e.
  \(\ell(\mathbf x; \theta) = \sum_{ij} \ell'(\mathbf x_{ij};\theta)\),
  then its gradient will be a sum over the gradients of each of its
  spatial components. Therefore: SGD on whole image \(=\) SGD on sum of
  each receptive field. Thus we can do mini-batch on each of the final
  layer receptive fields.
\end{itemize}

When these receptive fields overlap significantly, feedforward and
backpropagation are much more efficient layer-by-layer instead of
patch-by-patch.

However, there is a problem. We want \emph{dense predictions}, a
prediction of a probability of a certain class (softmax, One-Hot
encoding). Instead, we get \emph{coarse predictions}, a heatmap as
output.

So, how to convert these classical nets with dense layers to FCNs with
coarse layers? We still want pixelwise prediction, so we need to
transform the coarse output back to pixels.

\begin{itemize}
\item
  We can use fast scanning, elaborated in 3.2.
\item
  We can use deconvolution layers for upsampling, elaborated in Section
  3.3.
\item
  We can use patchwise sampling, shown in Section 3.4.
\end{itemize}

\subparagraph{3.1. Adaping classifiers for dense
networks}\label{header-n123}

\textbf{Idea:} We can consider fully connected layer as a convolution
with a kernel of the entire image, as all neurons are connected to the
whole image creating a massive convolution.

\begin{itemize}
\item
  If we consider the networks this way, then these layers transform into
  a \emph{fully convolutional layer} that take input of any size and
  output classification maps.
\item
  We can use convolution loss functions layer-by-layer to speedup the
  computation considerably.
\end{itemize}

For example, it's considerly faster when using AlexNet.

This reinterpretation to FCNs yields output maps for inputs of any size,
but generally the output dimensions are reduced by subsampling.

\begin{itemize}
\item
  Keep filters small, make computation reasonable.
\end{itemize}

\subparagraph{3.2 Shift-and-stitch is filter
rarefaction}\label{header-n141}

\emph{Rarefaction} is similar to \emph{coarsening}, making less dense.

\textbf{Goal:} Connect coarse convolutional layer to dense pixels.

\begin{quote}
I don't fully understand this part.
\end{quote}

The idea here is to upsample dense predictions, done as follows:

\begin{itemize}
\item
  Increase the number of pixels by shifting them a few places.
\item
  Process all these \(f^2\) inputs (\(f =\) downsample factor) and
  interlace the outputs so that the predictions correspond to the pixels
  at the \emph{center} of their receptive fields.
\item
  Instead of increasing the cost by \(f^2\), we can use an a trous
  algorithm.
\end{itemize}

\subparagraph{3.3 Upsampling is backwards strided
convolution}\label{header-n161}

\textbf{Goal:} Connect coarse convolutional layer to dense pixels.

What about simply upsampling by interpolation?

For example, simple linear interpolation:

\begin{itemize}
\item
  Each output \(y_{ij}\) from the nearest four inputs by a linear map
  that depends only on \emph{relative positions} of input and output
  cells.
\end{itemize}

Upsampling with factor \(f\) is like convolution with a \emph{fractional
input stride} of \(\frac 1 f\).

\begin{itemize}
\item
  Called \textbf{backwards convolution} or \textbf{deconvolution} with
  an output stride of \(f\).
\item
  Forwardpass is similar to a backward pass in a convo-network and vice
  versa.
\item
  Note that it doesn't have to be biliniar upsampling, but this can be
  learned by the network. (How?)
\end{itemize}

\subparagraph{3.4 Patchwise training is loss
sampling}\label{header-n184}

Here's an idea: what if instead of upsampling we perform training on
patches on the image. These patches consists of all receptive fields of
the units below the loss for an image.

Still, full convolutional training is similar to patch-training, as the
patches in such FCNs are the local receptive fields of the units below
the loss for an image.

But, it has some advantages over FCNs:

\begin{itemize}
\item
  More efficient than uniform sampling of batches.

  \begin{itemize}
  \item
    Con: Reduces the number of batches. 
  \end{itemize}
\end{itemize}

\begin{itemize}
\item
  Can correct class imbalance.

  \begin{itemize}
  \item
    Con: Fully FCNs can mitigate this by weighting the loss, however.
  \end{itemize}
\item
  Can mitigate the spatial correlation of dense patches.

  \begin{itemize}
  \item
    Con: Fully FCNs can use loss sampling to address spatial
    correlation.
  \end{itemize}
\item
  If the patches have overlap, then using FCNs can still speedup
  computation.

  \begin{itemize}
  \item
    Con: Fully FCNs have more of a computational speedup.
  \end{itemize}
\end{itemize}

\paragraph{4. Segmentation Architecture}\label{header-n221}

Here we change succesful ILSVRC (ImageNet) classifiers to FCNs. These
are then used for segmentation and are used for the PASCAL VOC 2011
segmentation challenge.

\subparagraph{4.1 From classifier to dense FCN}\label{header-n224}

They consider three different classifiers, of which we will treat VGG
16-layer net in depth. The nets are transformed to FCNs as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Discard the final classifier layer.
\item
  Convert all fully connected layers to convoultions.
\item
  Append \(1 \times 1\) convolution layer with 21 features to predict
  scores for each of the PASCAL classes.
\end{enumerate}

\subparagraph{4.2 Combining What and Where}\label{header-n237}

\textbf{Problem:} We throw detailed information away if we keep
downsampling using pooling layers.

\begin{itemize}
\item
  The detailed information is the \emph{what}, whereas the shallow
  information is \emph{where}.
\item
  This limits the scale of detail in the upsampled output.
\end{itemize}

\textbf{Solution:} Add skips after certain shallow layers to combine
both information!

\begin{itemize}
\item
  Turns the topology from line \(\rightarrow\) DAG.
\item
  Combines fine and coarse layers \(\implies\) can make local
  predictions that respect global structure.
\end{itemize}

We compare three different architecture:

\begin{itemize}
\item
  FCN-32s, with no skip layers. Upsamples stride 32.
\item
  FCN-16s, combine predictions from both final layer and stride 16 layer
  for finer details.
\item
  FCN-8s, additional predictions from the stride 8 layer for further
  precision.
\end{itemize}

\subsection{Recap Neural Networks}\label{header-n271}

\subsubsection{Stanford Tutorial}\label{header-n272}

\paragraph{Linear Regression}\label{header-n273}

Goal: Predict a target value \(y\) starting from a vector of input
values \(x \in \mathbb R^n\). (\emph{Regression})

\begin{itemize}
\item
  Example: predict housing prices given the features (e.g. \# of
  bedrooms, garden?) of a house.
\item
  Features of \(i\)-th example is denoted as \(x^{(i)}\). 
\item
  How do we do this? Find a function \(h\), where \(y=h(x)\), such that
  \(y^{(i)} \approx h(x^{(i)})\).
\end{itemize}

We use the function \(h_\theta(x) = \sum_j\theta_jx_j = \theta^Tx\),
which is a linear function; a sum of a parameter for each feature
\(\theta_j\) times the feature \(x_j\) (whether it's in there, for
example).

In particular, we want to minimize a certain \emph{cost function}, that
measures how close \(y^{(i)} \approx h(x^{(i)})\):

\[J(\theta) = \frac 1 2 \sum_i \left(h_\theta(x^{(i)}) - y^{(i)}\right)^2=\frac 1 2 \sum_i \left( \theta^Tx^{(i)} - y^{(i)} \right )^2\]

\begin{itemize}
\item
  This is the \emph{mean-squared error} divided by 2 for easy
  differencing.
\end{itemize}

We use Gradient Descent to minimize this function, which takes small
steps in the opposite direciton of the derivative of the cost with
respect to each variable.

Therefore, to minimize the cost function, we need to know the cost
\(J(\theta)\) as well as its derivative \(\nabla_tJ(\theta)\) with
respect to each \(\theta_j\). Here, the derivative for each \(\theta_j\)
is:

\[\frac{\partial J(\theta)}{\partial \theta_j} = \sum_{i} x_j^{(i)}\left( h_\theta(x^{(i)}) - y^{(i)}\right)\]

\paragraph{Logistic regression}\label{header-n300}

Goal: predict a discrete variable instead of continuous.
(\emph{Classification})

A linear function does not make much sense, as the distances between
classes, say 0 and 1, don't have the same meaning as a linear distance.
Another idea: squash the high and low values from \(\theta^T x\) into a
probability from \(0\) to \(1\)! This can be done by the sigmoid
function:

\[P(y=1|x) = h_\theta(x) = \frac 1 {1 + e^{-\theta^Tx}} \equiv \sigma(\theta^Tx) \\
P(y=0|x) =1- h_\theta(x) = 1-\sigma(\theta^Tx)\]

However, this changes the cost we have to use. This is done by
maximizing the \emph{log likelihood}.

\[J(\theta) = - \sum_i\left(y^{(i)}\log(h_\theta(x^{(i)})) + (1 - y^{(i)}) \log(1-h_\theta(x^{(i)}))\right)\]

\begin{itemize}
\item
  Notice that only one of the two terms in the summation is non-zero.
\item
  We can check to which class an input \(x\) belongs by checking whether
  \(h_t(x) > 0.5\), since then \(P(y=1 | x) > P(y=0 | x)\).
\end{itemize}

The partial derivative of the cost with respect to the variable is:

\[\frac{\partial J(\theta)}{\partial \theta_j} \sum_i x_j^{(i)}(h_\theta(x^{(i)})-y^{(i)})\]

\begin{itemize}
\item
  This is essentially the same as the gradient for linear regression,
  except that now \(h_\theta(x) = \sigma(\theta^Tx)\).
\end{itemize}

\paragraph{Neural networks}\label{header-n323}

...

\paragraph{Convolutional network}\label{header-n326}

\textbf{Problem:} Using fully connected neural networks works well on
small images, but on large images this becomes computationally
unfeasible. Suppose we have a \(96 \times 96\) input image and we want
to learn \(100\) features. Then we need \(10^6\) parameters and training
would be \(\pm100\) times slower!

\textbf{Simple solution:} Restrict the connections between input
-\/-\textgreater{} hidden units to connect only a small subset of input
units. Say only \(3 \times 3\). This is similar to how biology does it.

Property of an image: due to spatiality, it has the property of
\emph{stationary}. This means that any features that we learn at one
part of the image can be applied to other parts as well.

\begin{itemize}
\item
  More concretely, if we learn a feature somewhere in the image, then we
  can apply this everywhere in the image.
\end{itemize}

\begin{itemize}
\item
  As an example, suppose we have a \(96\times 96\) image and we learn
  features of patches from \(8 \times 8\). This would result in a
  \(89 \times 89\) feature if the \emph{stride} is 1. Suppose we have
  \(100\) features that we want to learn. Now we only have
  \(8 \times 8 \times 100 = 6400\) paramters instead of \(1\ 000\ 000\)
  parameters!
\end{itemize}

\paragraph{Autoencoders}\label{header-n341}

What if you only have unlabeled training examples? Then we can use an
\textbf{autoencoder} neural network.

\textbf{Goal:} Learn from the structure of the data.

\begin{itemize}
\item
  The autoencoder tries to learn a function \(h_{W,b}(x) \approx x\).

  \begin{itemize}
  \item
    This is generally very easy to do, but we place some constraints on
    the network such as constraining the number of hidden nodes. The
    network is forced to learn a \emph{compressed} representation of the
    input.
  \item
    Another constraint we can impose on the network is a \emph{sparsity}
    constraint.

    \begin{itemize}
    \item
      We want to constraint the neurons to be \emph{inactive} most of
      the time.
    \item
      A measure for the sparsity of the network is as follows:

      \[\hat p_j = \frac 1 m \sum_{i=1}^m\left[a_j^{(2)}(x^{(i)})\right]\]

      We would like to (approximately) enforce that:

      \[\hat \rho _j = \rho\]

      Where \(\rho\) is a sparsitiy parameter, such as \(\rho=0.05\). 
    \item
      This can be achieved by adding an extra penalty on the cost, based
      on the \emph{Kullback-Leibler (KL)} divergence.
    \end{itemize}
  \end{itemize}
\end{itemize}

\end{document}
